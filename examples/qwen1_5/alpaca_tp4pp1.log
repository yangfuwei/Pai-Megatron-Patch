torchrun --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 44089 ../llama2/finetune_megatron_llama_withGA.py --save /home/fuweiy/Pai-Megatron-Patch/cache/checkpoints/qwen7B-test_tp4pp1_variance/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-4-pp-1-ac-sel-do-true-sp-false-tt--wt- --split 98,2,0 --train-data-path /home/fuweiy/Pai-Megatron-Patch/cache/datasets/alpaca_zh-qwen-train.json --valid-data-path /home/fuweiy/Pai-Megatron-Patch/cache/datasets/alpaca_zh-qwen-valid.json --test-data-path /home/fuweiy/Pai-Megatron-Patch/cache/datasets/alpaca_zh-qwen-valid.json --lr 1e-5 --min-lr 1e-6 --lr-decay-style linear --adam-beta1 0.9 --adam-beta2 0.95 --weight-decay 0.1 --clip-grad 1.0 --init-method-std 0.006 --dataloader-type cyclic --lr-decay-iters 1990 --lr-warmup-iters 10 --train-iters 2000 --micro-batch-size 1 --global-batch-size 128 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --ffn-hidden-size 11008 --seq-length 128 --max-position-embeddings 128 --max-padding-length 128 --log-interval 1 --eval-interval 10000 --eval-iters 10 --save-interval 1000 --tensorboard-queue-size 1 --tensorboard-dir /home/fuweiy/Pai-Megatron-Patch/cache/checkpoints/qwen7B-test_tp4pp1_variance/tensorboard/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-4-pp-1-ac-sel-do-true-sp-false-tt--wt-_2024.07.14-15.58.10 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensor-model-parallel-size 4 --pipeline-model-parallel-size 1 --dataset LLama-Pretrain-Raw --no-save-optim --no-load-optim --no-load-rng --num-workers 8 --seed 1234 --extra-vocab-size 293 --patch-tokenizer-type LLamaTokenizer --swiglu --normalization RMSNorm --use-llama2-rotary-position-embeddings --position-embedding-type rope --untie-embeddings-and-output-weights --rotary-base 1000000 --rotary-scale-factor 1 --no-gradient-accumulation-fusion --bf16 --load /home/fuweiy/Pai-Megatron-Patch/cache/models/megatron/Qwen1.5-7B-megatron-tp4-pp1 --recompute-activations --use-distributed-optimizer
W0714 15:58:11.472772 139664506962560 torch/distributed/run.py:778] 
W0714 15:58:11.472772 139664506962560 torch/distributed/run.py:778] *****************************************
W0714 15:58:11.472772 139664506962560 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0714 15:58:11.472772 139664506962560 torch/distributed/run.py:778] *****************************************
Zarr-based strategies will not be registered because of missing packages
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
> setting tensorboard ...
using world size: 8, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:NullTokenizer
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adaptive_seq_len ................................ False
  add_bias_attn_fc ................................ True
  add_bias_linear ................................. True
  add_bias_linear_fc .............................. True
  add_position_embedding .......................... True
  add_qkv_bias_linear ............................. False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_head_type ............................. None
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  convert_checkpoint_from_megatron_to_transformers  False
  cvcuda_image_processing ......................... False
  data_cache_path ................................. None
  data_dir ........................................ None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... None
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. cyclic
  dataset ......................................... LLama-Pretrain-Raw
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embed_layernorm ................................. False
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... False
  enable_parallel_output .......................... True
  enable_shared_expert ............................ False
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 128
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  epochs .......................................... None
  eval_dev ........................................ False
  eval_fp32 ....................................... False
  eval_interval ................................... 10000
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 2
  expert_model_parallel_size ...................... 1
  expert_tensor_parallelism ....................... False
  extra_vocab_size ................................ 293
  ffn_hidden_size ................................. 11008
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  freeze_clip_vision_tower ........................ False
  freeze_llm ...................................... False
  generation_length ............................... None
  global_batch_size ............................... 128
  glu_activation .................................. None
  gradient_accumulation_fusion .................... False
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 4096
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  image_aspect_ratio .............................. square
  image_folder .................................... 
  image_size ...................................... None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  input_len ....................................... 1
  intermediate_size ............................... None
  iter_per_epoch .................................. 1250
  keep_last ....................................... False
  kv_channels ..................................... 128
  kv_lora_rank .................................... None
  lazy_mpu_init ................................... None
  load ............................................ /home/fuweiy/Pai-Megatron-Patch/cache/models/megatron/Qwen1.5-7B-megatron-tp4-pp1
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 1e-05
  lr_decay_iters .................................. 1990
  lr_decay_samples ................................ None
  lr_decay_style .................................. linear
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 10
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_padding_length .............................. 128
  max_position_embeddings ......................... 128
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  mm_projector_type ............................... None
  mm_use_im_patch_token ........................... False
  mm_use_im_start_end ............................. False
  mm_vision_select_layer .......................... None
  mock_data ....................................... False
  moe ............................................. False
  moe_aux_loss_coeff .............................. 0.0
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ None
  moe_ffn_hidden_size ............................. None
  moe_grouped_gemm ................................ False
  moe_input_feature_slicing ....................... False
  moe_input_jitter_eps ............................ None
  moe_layer_freq .................................. 1
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dropping .............................. False
  moe_topk ........................................ 1
  moe_train_capacity_factor ....................... 1.0
  moe_z_loss_coeff ................................ None
  n_head_kv ....................................... None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... True
  no_load_rng ..................................... True
  no_persist_layer_norm ........................... False
  no_save_optim ................................... True
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_fewshot ..................................... None
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_shared_experts .............................. None
  num_workers ..................................... 8
  one_logger_entity ............................... hwinf_dcm
  one_logger_project .............................. e2e-tracking
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  out_seq_length .................................. 1024
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  patch_size ...................................... None
  patch_tokenizer_type ............................ LLamaTokenizer
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  position_encoding_2d ............................ False
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  q_lora_rank ..................................... None
  qk_nope_head_dim ................................ None
  qk_rope_head_dim ................................ None
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  repetition_penalty .............................. 1.1
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_verify_neighbor_count ..................... True
  retro_workdir ................................... None
  rotary_base ..................................... 1000000
  rotary_percent .................................. 1.0
  rotary_scale_factor ............................. 1
  rotary_scaling_factor ........................... 1
  rotary_seq_len_interpolation_factor ............. None
  router_type ..................................... topk
  sample_rate ..................................... 1.0
  save ............................................ /home/fuweiy/Pai-Megatron-Patch/cache/checkpoints/qwen7B-test_tp4pp1_variance/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-4-pp-1-ac-sel-do-true-sp-false-tt--wt-
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 128
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  shared_moe_ffn_hidden_size ...................... None
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  sliding_window .................................. None
  source_seq_len .................................. None
  spec ............................................ None
  split ........................................... 98,2,0
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  target_seq_len .................................. None
  task_list ....................................... all
  temperature ..................................... 1.0
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /home/fuweiy/Pai-Megatron-Patch/cache/checkpoints/qwen7B-test_tp4pp1_variance/tensorboard/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-4-pp-1-ac-sel-do-true-sp-false-tt--wt-_2024.07.14-15.58.10
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  test_data_path .................................. ['/home/fuweiy/Pai-Megatron-Patch/cache/datasets/alpaca_zh-qwen-valid.json']
  text_generate_gt_file ........................... 
  text_generate_input_file ........................ 
  text_generate_output_file ....................... 
  time ............................................ False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. NullTokenizer
  top_k ........................................... 0
  top_p ........................................... 0.0
  torch_compile ................................... True
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_cfg ............................. None
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data ...................................... None
  train_data_path ................................. ['/home/fuweiy/Pai-Megatron-Patch/cache/datasets/alpaca_zh-qwen-train.json']
  train_iters ..................................... 2000
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  transformer_timers .............................. False
  transformer_type ................................ megatron
  tune_mm_mlp_adapter ............................. False
  untie_embeddings_and_output_weights ............. True
  use_alibi_mask .................................. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. False
  use_llama2_rotary_position_embeddings ........... True
  use_mcore_models ................................ False
  use_mistral_rotary_position_embeddings .......... False
  use_normhead .................................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tutel ....................................... False
  v_head_dim ...................................... None
  valid_data ...................................... None
  valid_data_path ................................. ['/home/fuweiy/Pai-Megatron-Patch/cache/datasets/alpaca_zh-qwen-valid.json']
  variable_seq_lengths ............................ False
  verbosity ....................................... INFO
  version ......................................... plain
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vision_tower .................................... 
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... -1
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 8
  z_loss_weight ................................... 0.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 64
> building NullTokenizer tokenizer ...
 > padded vocab (size: 0) with 0 dummy tokens (new size: 0)
> initializing torch distributed ...
> initialized tensor model parallel with size 4
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/home/fuweiy/Pai-Megatron-Patch/Megatron-LM/megatron/core/datasets'
g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/opt/conda/envs/py_3.9/include/python3.9 -I/opt/conda/envs/py_3.9/lib/python3.9/site-packages/pybind11/include helpers.cpp -o helpers.cpython-39-x86_64-linux-gnu.so
make: Leaving directory '/home/fuweiy/Pai-Megatron-Patch/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 3.971 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 1.609 seconds
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
/home/fuweiy/Pai-Megatron-Patch/megatron_patch/training.py:100: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /var/lib/jenkins/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  start_time_tensor = torch.cuda.DoubleTensor([_TRAIN_START_TIME])
time to initialize megatron (seconds): 10.793
[after megatron is initialized] datetime: 2024-07-14 15:58:26 
> building LLamaTokenizer tokenizer ...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1930969088
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1930969088
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1930969088
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1930969088
INFO:megatron.core.distributed.grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
INFO:megatron.core.distributed.grad_buffer:Params for bucket 1 (1930969088 elements):
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.output_layer.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.embedding.word_embeddings.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.24.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.17.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.13.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.15.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.26.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.2.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.28.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.30.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.4.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.14.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.21.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.18.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.6.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.post_attention_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.22.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.3.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.final_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.10.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.mlp.dense_h_to_4h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.27.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.23.self_attention.dense.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.20.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.1.input_norm.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.0.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.31.self_attention.dense.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.29.mlp.dense_4h_to_h.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.25.self_attention.query_key_value.weight
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.11.self_attention.query_key_value.bias
INFO:megatron.core.distributed.grad_buffer:    module.language_model.encoder.layers.9.self_attention.dense.weight
> learning rate decay style: linear
 loading release checkpoint from /home/fuweiy/Pai-Megatron-Patch/cache/models/megatron/Qwen1.5-7B-megatron-tp4-pp1
 checkpoint version 3.0
  successfully loaded checkpoint from /home/fuweiy/Pai-Megatron-Patch/cache/models/megatron/Qwen1.5-7B-megatron-tp4-pp1 at iteration 0
(min, max) time across ranks (ms):
    load-checkpoint ................................: (4241.41, 4241.54)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-07-14 15:58:30 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      256000
    validation: 1280
    test:       1280
> building LLamaTokenizer tokenizer ...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 50151 examples [00:00, 260619.10 examples/s]Generating train split: 50151 examples [00:00, 242979.47 examples/s]
Running Encoding (num_proc=16):   0%|          | 0/50151 [00:00<?, ? examples/s]Running Encoding (num_proc=16):   0%|          | 0/50151 [00:00<?, ? examples/s]Running Encoding (num_proc=16):   6%|▌         | 3000/50151 [00:08<02:05, 374.74 examples/s]Running Encoding (num_proc=16):   6%|▌         | 3000/50151 [00:08<02:07, 368.89 examples/s]Running Encoding (num_proc=16):   6%|▋         | 3135/50151 [00:08<02:03, 380.00 examples/s]Running Encoding (num_proc=16):   6%|▋         | 3135/50151 [00:08<02:05, 375.06 examples/s]Running Encoding (num_proc=16):  12%|█▏        | 6135/50151 [00:09<00:53, 817.96 examples/s]Running Encoding (num_proc=16):  12%|█▏        | 6135/50151 [00:09<00:54, 810.34 examples/s]Running Encoding (num_proc=16):  13%|█▎        | 6270/50151 [00:09<00:54, 800.61 examples/s]Running Encoding (num_proc=16):  13%|█▎        | 6270/50151 [00:10<00:55, 792.68 examples/s]Running Encoding (num_proc=16):  18%|█▊        | 9270/50151 [00:11<00:33, 1211.66 examples/s]Running Encoding (num_proc=16):  19%|█▉        | 9405/50151 [00:11<00:35, 1137.68 examples/s]Running Encoding (num_proc=16):  18%|█▊        | 9270/50151 [00:11<00:35, 1159.08 examples/s]Running Encoding (num_proc=16):  19%|█▉        | 9405/50151 [00:12<00:37, 1087.74 examples/s]Running Encoding (num_proc=16):  25%|██▍       | 12405/50151 [00:13<00:23, 1572.96 examples/s]Running Encoding (num_proc=16):  25%|██▍       | 12405/50151 [00:13<00:24, 1550.28 examples/s]Running Encoding (num_proc=16):  31%|███       | 15540/50151 [00:14<00:20, 1687.56 examples/s]Running Encoding (num_proc=16):  31%|███       | 15540/50151 [00:14<00:20, 1672.34 examples/s]Running Encoding (num_proc=16):  37%|███▋      | 18675/50151 [00:16<00:19, 1631.14 examples/s]Running Encoding (num_proc=16):  37%|███▋      | 18675/50151 [00:17<00:19, 1613.57 examples/s]Running Encoding (num_proc=16):  43%|████▎     | 21810/50151 [00:18<00:16, 1703.01 examples/s]Running Encoding (num_proc=16):  43%|████▎     | 21810/50151 [00:18<00:16, 1699.85 examples/s]Running Encoding (num_proc=16):  50%|████▉     | 24945/50151 [00:20<00:14, 1712.49 examples/s]Running Encoding (num_proc=16):  50%|████▉     | 24945/50151 [00:20<00:14, 1694.78 examples/s]Running Encoding (num_proc=16):  56%|█████▌    | 28079/50151 [00:21<00:12, 1745.76 examples/s]Running Encoding (num_proc=16):  56%|█████▌    | 28079/50151 [00:22<00:12, 1738.22 examples/s]Running Encoding (num_proc=16):  62%|██████▏   | 31213/50151 [00:23<00:10, 1774.79 examples/s]Running Encoding (num_proc=16):  62%|██████▏   | 31213/50151 [00:24<00:11, 1702.12 examples/s]Running Encoding (num_proc=16):  68%|██████▊   | 34347/50151 [00:25<00:08, 1842.65 examples/s]Running Encoding (num_proc=16):  68%|██████▊   | 34347/50151 [00:25<00:08, 1819.87 examples/s]Running Encoding (num_proc=16):  75%|███████▍  | 37481/50151 [00:27<00:07, 1660.80 examples/s]Running Encoding (num_proc=16):  75%|███████▍  | 37481/50151 [00:27<00:07, 1663.81 examples/s]Running Encoding (num_proc=16):  81%|████████  | 40615/50151 [00:28<00:05, 1800.48 examples/s]Running Encoding (num_proc=16):  81%|████████  | 40615/50151 [00:29<00:05, 1813.50 examples/s]Running Encoding (num_proc=16):  87%|████████▋ | 43749/50151 [00:30<00:03, 1771.21 examples/s]Running Encoding (num_proc=16):  87%|████████▋ | 43749/50151 [00:31<00:03, 1739.24 examples/s]Running Encoding (num_proc=16):  93%|█████████▎| 46883/50151 [00:32<00:01, 1809.30 examples/s]Running Encoding (num_proc=16):  93%|█████████▎| 46883/50151 [00:32<00:01, 1768.78 examples/s]Running Encoding (num_proc=16): 100%|█████████▉| 50017/50151 [00:34<00:00, 1809.79 examples/s]Running Encoding (num_proc=16): 100%|██████████| 50151/50151 [00:34<00:00, 1452.49 examples/s]
Running Encoding (num_proc=16): 100%|█████████▉| 50017/50151 [00:34<00:00, 1814.92 examples/s]Running Encoding (num_proc=16): 100%|██████████| 50151/50151 [00:34<00:00, 1435.79 examples/s]
0it [00:00, ?it/s]0it [00:00, ?it/s]33365it [00:00, 333626.46it/s]50151it [00:00, 336311.75it/s]
  >> total number of samples: 45413
10154it [00:00, 41381.93it/s]44217it [00:00, 151492.18it/s]50151it [00:00, 138335.59it/s]
  >> total number of samples: 45413
[after dataloaders are built] datetime: 2024-07-14 15:59:12 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (4564.48, 4567.24)
    train/valid/test-data-iterators-setup ..........: (42114.81, 42116.66)
[before the start of training step] datetime: 2024-07-14 15:59:13 
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0626 ms 100.0%
  triton_mm_17 0.0933 ms 67.1%
  triton_mm_11 0.1217 ms 51.4%
  triton_mm_0 0.1243 ms 50.4%
  triton_mm_16 0.1253 ms 50.0%
  triton_mm_1 0.1326 ms 47.2%
  triton_mm_13 0.1385 ms 45.2%
  triton_mm_23 0.1566 ms 40.0%
  triton_mm_22 0.1582 ms 39.6%
  triton_mm_5 0.1591 ms 39.4%
SingleProcess AUTOTUNE benchmarking takes 18.9365 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0631 ms 100.0%
  triton_mm_17 0.0929 ms 67.9%
  triton_mm_11 0.1212 ms 52.0%
  triton_mm_0 0.1246 ms 50.6%
  triton_mm_16 0.1261 ms 50.0%
  triton_mm_1 0.1342 ms 47.0%
  triton_mm_13 0.1380 ms 45.7%
  triton_mm_23 0.1576 ms 40.0%
  triton_mm_5 0.1588 ms 39.7%
  triton_mm_22 0.1593 ms 39.6%
SingleProcess AUTOTUNE benchmarking takes 21.2459 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0627 ms 100.0%
  triton_mm_17 0.0947 ms 66.2%
  triton_mm_11 0.1220 ms 51.4%
  triton_mm_0 0.1250 ms 50.1%
  triton_mm_16 0.1254 ms 50.0%
  triton_mm_1 0.1334 ms 47.0%
  triton_mm_13 0.1390 ms 45.1%
  triton_mm_23 0.1565 ms 40.1%
  triton_mm_22 0.1583 ms 39.6%
  triton_mm_5 0.1586 ms 39.5%
SingleProcess AUTOTUNE benchmarking takes 21.3070 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0627 ms 100.0%
  triton_mm_17 0.0915 ms 68.5%
  triton_mm_11 0.1217 ms 51.5%
  triton_mm_16 0.1259 ms 49.8%
  triton_mm_0 0.1259 ms 49.7%
  triton_mm_1 0.1333 ms 47.0%
  triton_mm_13 0.1386 ms 45.2%
  triton_mm_23 0.1574 ms 39.8%
  triton_mm_22 0.1590 ms 39.4%
  triton_mm_5 0.1600 ms 39.1%
SingleProcess AUTOTUNE benchmarking takes 22.3251 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0625 ms 100.0%
  triton_mm_17 0.0947 ms 66.1%
  triton_mm_11 0.1207 ms 51.8%
  triton_mm_0 0.1241 ms 50.4%
  triton_mm_16 0.1252 ms 49.9%
  triton_mm_1 0.1316 ms 47.5%
  triton_mm_13 0.1381 ms 45.3%
  triton_mm_23 0.1566 ms 39.9%
  triton_mm_22 0.1583 ms 39.5%
  triton_mm_5 0.1589 ms 39.4%
SingleProcess AUTOTUNE benchmarking takes 21.9577 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0627 ms 100.0%
  triton_mm_17 0.0945 ms 66.4%
  triton_mm_11 0.1219 ms 51.4%
  triton_mm_0 0.1233 ms 50.9%
  triton_mm_16 0.1253 ms 50.0%
  triton_mm_1 0.1334 ms 47.0%
  triton_mm_13 0.1389 ms 45.1%
  triton_mm_23 0.1565 ms 40.1%
  triton_mm_22 0.1581 ms 39.7%
  triton_mm_5 0.1593 ms 39.4%
SingleProcess AUTOTUNE benchmarking takes 23.9853 seconds and 0.0000 seconds precompiling
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0630 ms 100.0%
  triton_mm_17 0.0947 ms 66.5%
  triton_mm_11 0.1244 ms 50.6%
  triton_mm_0 0.1249 ms 50.4%
  triton_mm_16 0.1252 ms 50.3%
  triton_mm_1 0.1334 ms 47.2%
  triton_mm_13 0.1379 ms 45.7%
  triton_mm_23 0.1568 ms 40.2%
  triton_mm_22 0.1590 ms 39.6%
  triton_mm_5 0.1592 ms 39.6%
SingleProcess AUTOTUNE benchmarking takes 24.8851 seconds and 0.0000 seconds precompiling
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
AUTOTUNE mm(128x4096, 4096x3072)
  mm 0.0626 ms 100.0%
  triton_mm_17 0.0946 ms 66.2%
  triton_mm_0 0.1239 ms 50.5%
  triton_mm_11 0.1241 ms 50.5%
  triton_mm_16 0.1254 ms 49.9%
  triton_mm_1 0.1333 ms 47.0%
  triton_mm_13 0.1384 ms 45.2%
  triton_mm_5 0.1586 ms 39.5%
  triton_mm_23 0.1605 ms 39.0%
  triton_mm_22 0.1607 ms 39.0%
SingleProcess AUTOTUNE benchmarking takes 26.4927 seconds and 0.0000 seconds precompiling
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/opt/conda/envs/py_3.9/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:123: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_37 0.0133 ms 100.0%
  triton_bmm_35 0.0138 ms 96.2%
  triton_bmm_41 0.0152 ms 87.3%
  bmm 0.0177 ms 74.9%
  triton_bmm_45 0.0177 ms 74.9%
  triton_bmm_24 0.0178 ms 74.5%
  triton_bmm_25 0.0180 ms 73.7%
  triton_bmm_42 0.0198 ms 66.9%
  triton_bmm_40 0.0199 ms 66.7%
  triton_bmm_43 0.0201 ms 65.9%
SingleProcess AUTOTUNE benchmarking takes 22.4793 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_37 0.0136 ms 100.0%
  triton_bmm_35 0.0146 ms 93.1%
  triton_bmm_41 0.0158 ms 85.8%
  triton_bmm_45 0.0171 ms 79.4%
  bmm 0.0178 ms 76.3%
  triton_bmm_24 0.0181 ms 75.0%
  triton_bmm_25 0.0184 ms 73.7%
  triton_bmm_43 0.0194 ms 69.9%
  triton_bmm_40 0.0200 ms 68.1%
  triton_bmm_42 0.0202 ms 67.4%
SingleProcess AUTOTUNE benchmarking takes 23.7556 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_37 0.0134 ms 100.0%
  triton_bmm_35 0.0144 ms 93.0%
  triton_bmm_41 0.0154 ms 86.7%
  triton_bmm_25 0.0170 ms 78.8%
  triton_bmm_24 0.0175 ms 76.4%
  bmm 0.0175 ms 76.3%
  triton_bmm_45 0.0175 ms 76.3%
  triton_bmm_40 0.0193 ms 69.4%
  triton_bmm_42 0.0197 ms 68.0%
  triton_bmm_31 0.0200 ms 67.1%
SingleProcess AUTOTUNE benchmarking takes 22.4863 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_35 0.0140 ms 100.0%
  triton_bmm_37 0.0145 ms 96.7%
  triton_bmm_41 0.0153 ms 91.4%
  triton_bmm_24 0.0177 ms 79.0%
  triton_bmm_45 0.0180 ms 77.7%
  triton_bmm_25 0.0180 ms 77.6%
  bmm 0.0184 ms 76.0%
  triton_bmm_40 0.0199 ms 70.2%
  triton_bmm_42 0.0200 ms 70.1%
  triton_bmm_43 0.0202 ms 69.1%
SingleProcess AUTOTUNE benchmarking takes 23.6810 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_37 0.0138 ms 100.0%
  triton_bmm_35 0.0147 ms 94.3%
  triton_bmm_41 0.0157 ms 88.0%
  triton_bmm_45 0.0170 ms 81.2%
  triton_bmm_25 0.0173 ms 79.9%
  bmm 0.0176 ms 78.6%
  triton_bmm_24 0.0179 ms 77.4%
  triton_bmm_42 0.0190 ms 72.8%
  triton_bmm_40 0.0193 ms 71.6%
  triton_bmm_31 0.0202 ms 68.6%
SingleProcess AUTOTUNE benchmarking takes 25.1251 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_37 0.0143 ms 100.0%
  triton_bmm_35 0.0147 ms 97.3%
  triton_bmm_41 0.0159 ms 89.9%
  bmm 0.0177 ms 80.7%
  triton_bmm_24 0.0180 ms 79.3%
  triton_bmm_45 0.0180 ms 79.3%
  triton_bmm_25 0.0183 ms 78.1%
  triton_bmm_40 0.0195 ms 73.0%
  triton_bmm_42 0.0201 ms 71.1%
  triton_bmm_43 0.0203 ms 70.1%
SingleProcess AUTOTUNE benchmarking takes 29.1415 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_35 0.0138 ms 100.0%
  triton_bmm_37 0.0141 ms 98.0%
  triton_bmm_41 0.0151 ms 91.3%
  triton_bmm_24 0.0176 ms 78.6%
  bmm 0.0177 ms 78.2%
  triton_bmm_45 0.0179 ms 77.4%
  triton_bmm_25 0.0179 ms 77.2%
  triton_bmm_40 0.0194 ms 71.3%
  triton_bmm_42 0.0198 ms 69.8%
  triton_bmm_31 0.0200 ms 69.0%
SingleProcess AUTOTUNE benchmarking takes 26.5888 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_37 0.0135 ms 100.0%
  triton_bmm_35 0.0140 ms 96.3%
  triton_bmm_41 0.0150 ms 89.9%
  triton_bmm_24 0.0168 ms 80.4%
  triton_bmm_25 0.0173 ms 78.2%
  bmm 0.0174 ms 77.5%
  triton_bmm_45 0.0176 ms 76.8%
  triton_bmm_40 0.0195 ms 69.2%
  triton_bmm_42 0.0198 ms 68.2%
  triton_bmm_43 0.0201 ms 67.3%
SingleProcess AUTOTUNE benchmarking takes 25.1634 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  mm 0.0307 ms 100.0%
  triton_mm_65 0.0330 ms 93.0%
  triton_mm_64 0.0413 ms 74.4%
  triton_mm_48 0.0425 ms 72.3%
  triton_mm_59 0.0457 ms 67.1%
  triton_mm_49 0.0468 ms 65.5%
  triton_mm_71 0.0508 ms 60.5%
  triton_mm_70 0.0516 ms 59.5%
  triton_mm_61 0.0516 ms 59.5%
  triton_mm_53 0.0554 ms 55.3%
SingleProcess AUTOTUNE benchmarking takes 15.7646 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  mm 0.0312 ms 100.0%
  triton_mm_65 0.0331 ms 94.5%
  triton_mm_64 0.0413 ms 75.7%
  triton_mm_59 0.0458 ms 68.2%
  triton_mm_48 0.0463 ms 67.5%
  triton_mm_49 0.0470 ms 66.5%
  triton_mm_71 0.0512 ms 61.1%
  triton_mm_61 0.0517 ms 60.4%
  triton_mm_70 0.0518 ms 60.3%
  triton_mm_50 0.0536 ms 58.3%
SingleProcess AUTOTUNE benchmarking takes 16.1998 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  triton_mm_65 0.0330 ms 100.0%
  mm 0.0339 ms 97.3%
  triton_mm_64 0.0411 ms 80.2%
  triton_mm_59 0.0454 ms 72.7%
  triton_mm_48 0.0461 ms 71.6%
  triton_mm_49 0.0464 ms 71.2%
  triton_mm_71 0.0510 ms 64.7%
  triton_mm_61 0.0516 ms 64.0%
  triton_mm_70 0.0517 ms 63.8%
  triton_mm_55 0.0550 ms 60.0%
SingleProcess AUTOTUNE benchmarking takes 17.5907 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  triton_mm_65 0.0308 ms 100.0%
  mm 0.0310 ms 99.4%
  triton_mm_64 0.0412 ms 74.7%
  triton_mm_59 0.0421 ms 73.2%
  triton_mm_48 0.0460 ms 67.0%
  triton_mm_49 0.0465 ms 66.2%
  triton_mm_71 0.0468 ms 65.9%
  triton_mm_70 0.0514 ms 59.9%
  triton_mm_61 0.0516 ms 59.7%
  triton_mm_57 0.0550 ms 56.0%
SingleProcess AUTOTUNE benchmarking takes 18.6370 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  triton_mm_65 0.0329 ms 100.0%
  mm 0.0346 ms 94.9%
  triton_mm_64 0.0409 ms 80.3%
  triton_mm_59 0.0455 ms 72.2%
  triton_mm_48 0.0460 ms 71.4%
  triton_mm_49 0.0466 ms 70.5%
  triton_mm_71 0.0507 ms 64.8%
  triton_mm_70 0.0512 ms 64.1%
  triton_mm_61 0.0516 ms 63.6%
  triton_mm_55 0.0531 ms 61.8%
SingleProcess AUTOTUNE benchmarking takes 23.3296 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  mm 0.0310 ms 100.0%
  triton_mm_65 0.0328 ms 94.5%
  triton_mm_64 0.0410 ms 75.6%
  triton_mm_59 0.0456 ms 68.0%
  triton_mm_48 0.0461 ms 67.3%
  triton_mm_49 0.0467 ms 66.4%
  triton_mm_71 0.0506 ms 61.2%
  triton_mm_70 0.0514 ms 60.3%
  triton_mm_61 0.0516 ms 60.1%
  triton_mm_57 0.0554 ms 55.9%
SingleProcess AUTOTUNE benchmarking takes 22.9263 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  triton_mm_65 0.0306 ms 100.0%
  mm 0.0324 ms 94.7%
  triton_mm_64 0.0411 ms 74.5%
  triton_mm_49 0.0433 ms 70.8%
  triton_mm_59 0.0453 ms 67.7%
  triton_mm_48 0.0458 ms 66.9%
  triton_mm_71 0.0507 ms 60.4%
  triton_mm_70 0.0512 ms 59.9%
  triton_mm_61 0.0512 ms 59.8%
  triton_mm_57 0.0538 ms 56.9%
SingleProcess AUTOTUNE benchmarking takes 20.3616 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x1024, 1024x4096)
  mm 0.0316 ms 100.0%
  triton_mm_65 0.0328 ms 96.3%
  triton_mm_64 0.0409 ms 77.4%
  triton_mm_59 0.0453 ms 69.8%
  triton_mm_48 0.0460 ms 68.7%
  triton_mm_49 0.0464 ms 68.1%
  triton_mm_71 0.0506 ms 62.5%
  triton_mm_70 0.0514 ms 61.5%
  triton_mm_61 0.0515 ms 61.4%
  triton_mm_55 0.0535 ms 59.1%
SingleProcess AUTOTUNE benchmarking takes 21.8691 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0899 ms 100.0%
  triton_mm_89 0.1147 ms 78.4%
  triton_mm_72 0.1477 ms 60.9%
  triton_mm_73 0.1534 ms 58.6%
  triton_mm_95 0.1585 ms 56.7%
  triton_mm_94 0.1602 ms 56.1%
  triton_mm_88 0.1663 ms 54.0%
  triton_mm_77 0.1931 ms 46.5%
  triton_mm_76 0.1997 ms 45.0%
  triton_mm_81 0.2068 ms 43.5%
SingleProcess AUTOTUNE benchmarking takes 20.3021 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0896 ms 100.0%
  triton_mm_89 0.1155 ms 77.6%
  triton_mm_72 0.1483 ms 60.4%
  triton_mm_73 0.1528 ms 58.6%
  triton_mm_94 0.1616 ms 55.4%
  triton_mm_95 0.1622 ms 55.2%
  triton_mm_88 0.1669 ms 53.7%
  triton_mm_77 0.1932 ms 46.4%
  triton_mm_76 0.2007 ms 44.6%
  triton_mm_81 0.2046 ms 43.8%
SingleProcess AUTOTUNE benchmarking takes 21.8178 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0893 ms 100.0%
  triton_mm_89 0.1151 ms 77.6%
  triton_mm_72 0.1485 ms 60.2%
  triton_mm_73 0.1522 ms 58.7%
  triton_mm_94 0.1597 ms 55.9%
  triton_mm_95 0.1613 ms 55.4%
  triton_mm_88 0.1662 ms 53.7%
  triton_mm_77 0.2001 ms 44.6%
  triton_mm_76 0.2030 ms 44.0%
  triton_mm_81 0.2041 ms 43.8%
SingleProcess AUTOTUNE benchmarking takes 22.9885 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0897 ms 100.0%
  triton_mm_89 0.1149 ms 78.1%
  triton_mm_72 0.1479 ms 60.7%
  triton_mm_73 0.1525 ms 58.8%
  triton_mm_95 0.1587 ms 56.5%
  triton_mm_94 0.1599 ms 56.1%
  triton_mm_88 0.1657 ms 54.1%
  triton_mm_77 0.1933 ms 46.4%
  triton_mm_76 0.2007 ms 44.7%
  triton_mm_81 0.2041 ms 44.0%
SingleProcess AUTOTUNE benchmarking takes 26.4847 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0893 ms 100.0%
  triton_mm_89 0.1153 ms 77.4%
  triton_mm_72 0.1479 ms 60.4%
  triton_mm_73 0.1537 ms 58.1%
  triton_mm_95 0.1583 ms 56.4%
  triton_mm_94 0.1598 ms 55.9%
  triton_mm_88 0.1672 ms 53.4%
  triton_mm_77 0.1942 ms 46.0%
  triton_mm_76 0.1976 ms 45.2%
  triton_mm_81 0.2068 ms 43.2%
SingleProcess AUTOTUNE benchmarking takes 25.5966 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0863 ms 100.0%
  triton_mm_89 0.1146 ms 75.3%
  triton_mm_72 0.1478 ms 58.4%
  triton_mm_73 0.1526 ms 56.6%
  triton_mm_95 0.1564 ms 55.2%
  triton_mm_94 0.1586 ms 54.4%
  triton_mm_88 0.1659 ms 52.0%
  triton_mm_77 0.1900 ms 45.4%
  triton_mm_76 0.2015 ms 42.8%
  triton_mm_81 0.2046 ms 42.2%
SingleProcess AUTOTUNE benchmarking takes 26.1516 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0899 ms 100.0%
  triton_mm_89 0.1151 ms 78.1%
  triton_mm_72 0.1478 ms 60.8%
  triton_mm_73 0.1516 ms 59.3%
  triton_mm_95 0.1580 ms 56.9%
  triton_mm_94 0.1601 ms 56.2%
  triton_mm_88 0.1662 ms 54.1%
  triton_mm_77 0.1955 ms 46.0%
  triton_mm_76 0.2006 ms 44.8%
  triton_mm_81 0.2041 ms 44.0%
SingleProcess AUTOTUNE benchmarking takes 26.0614 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x5504)
  mm 0.0899 ms 100.0%
  triton_mm_89 0.1151 ms 78.1%
  triton_mm_72 0.1469 ms 61.2%
  triton_mm_73 0.1525 ms 58.9%
  triton_mm_95 0.1599 ms 56.2%
  triton_mm_94 0.1603 ms 56.1%
  triton_mm_88 0.1668 ms 53.9%
  triton_mm_77 0.1956 ms 45.9%
  triton_mm_76 0.2008 ms 44.7%
  triton_mm_81 0.2043 ms 44.0%
SingleProcess AUTOTUNE benchmarking takes 27.0286 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0631 ms 100.0%
  triton_mm_113 0.0670 ms 94.1%
  triton_mm_96 0.0849 ms 74.3%
  triton_mm_112 0.0879 ms 71.8%
  triton_mm_97 0.0898 ms 70.2%
  triton_mm_107 0.0961 ms 65.6%
  triton_mm_105 0.0978 ms 64.5%
  triton_mm_103 0.1000 ms 63.0%
  triton_mm_118 0.1001 ms 63.0%
  triton_mm_104 0.1011 ms 62.4%
SingleProcess AUTOTUNE benchmarking takes 23.8131 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0635 ms 100.0%
  triton_mm_113 0.0672 ms 94.5%
  triton_mm_96 0.0854 ms 74.4%
  triton_mm_112 0.0886 ms 71.7%
  triton_mm_97 0.0898 ms 70.7%
  triton_mm_107 0.0965 ms 65.8%
  triton_mm_105 0.0967 ms 65.6%
  triton_mm_118 0.1006 ms 63.1%
  triton_mm_104 0.1008 ms 63.0%
  triton_mm_103 0.1012 ms 62.7%
SingleProcess AUTOTUNE benchmarking takes 24.0733 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0631 ms 100.0%
  triton_mm_113 0.0695 ms 90.9%
  triton_mm_96 0.0850 ms 74.3%
  triton_mm_112 0.0882 ms 71.6%
  triton_mm_97 0.0893 ms 70.7%
  triton_mm_107 0.0961 ms 65.7%
  triton_mm_105 0.0967 ms 65.3%
  triton_mm_104 0.0998 ms 63.3%
  triton_mm_118 0.1001 ms 63.1%
  triton_mm_103 0.1006 ms 62.8%
SingleProcess AUTOTUNE benchmarking takes 23.7000 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0631 ms 100.0%
  triton_mm_113 0.0669 ms 94.4%
  triton_mm_96 0.0850 ms 74.2%
  triton_mm_112 0.0881 ms 71.7%
  triton_mm_97 0.0891 ms 70.9%
  triton_mm_107 0.0960 ms 65.8%
  triton_mm_105 0.0969 ms 65.1%
  triton_mm_103 0.0998 ms 63.2%
  triton_mm_118 0.1001 ms 63.0%
  triton_mm_104 0.1005 ms 62.8%
SingleProcess AUTOTUNE benchmarking takes 27.4160 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0619 ms 100.0%
  triton_mm_113 0.0692 ms 89.4%
  triton_mm_96 0.0847 ms 73.1%
  triton_mm_112 0.0883 ms 70.1%
  triton_mm_97 0.0894 ms 69.2%
  triton_mm_107 0.0963 ms 64.3%
  triton_mm_105 0.0973 ms 63.6%
  triton_mm_103 0.0997 ms 62.1%
  triton_mm_104 0.1002 ms 61.8%
  triton_mm_118 0.1003 ms 61.7%
SingleProcess AUTOTUNE benchmarking takes 22.0038 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0629 ms 100.0%
  triton_mm_113 0.0690 ms 91.2%
  triton_mm_96 0.0846 ms 74.3%
  triton_mm_112 0.0879 ms 71.6%
  triton_mm_97 0.0890 ms 70.7%
  triton_mm_107 0.0958 ms 65.7%
  triton_mm_105 0.0968 ms 65.0%
  triton_mm_103 0.0996 ms 63.2%
  triton_mm_118 0.0999 ms 63.0%
  triton_mm_104 0.1000 ms 63.0%
SingleProcess AUTOTUNE benchmarking takes 22.9415 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0632 ms 100.0%
  triton_mm_113 0.0667 ms 94.7%
  triton_mm_96 0.0853 ms 74.1%
  triton_mm_112 0.0877 ms 72.0%
  triton_mm_97 0.0896 ms 70.5%
  triton_mm_107 0.0960 ms 65.8%
  triton_mm_105 0.0966 ms 65.4%
  triton_mm_118 0.0997 ms 63.3%
  triton_mm_104 0.1001 ms 63.1%
  triton_mm_103 0.1007 ms 62.8%
SingleProcess AUTOTUNE benchmarking takes 22.7362 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0137 ms 100.0%
  triton_bmm_131 0.0137 ms 99.4%
  triton_bmm_137 0.0152 ms 90.0%
  triton_bmm_141 0.0170 ms 80.4%
  triton_bmm_121 0.0173 ms 78.9%
  triton_bmm_120 0.0179 ms 76.5%
  bmm 0.0182 ms 75.1%
  triton_bmm_138 0.0189 ms 72.2%
  triton_bmm_139 0.0191 ms 71.3%
  triton_bmm_136 0.0192 ms 71.0%
SingleProcess AUTOTUNE benchmarking takes 2.9479 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0135 ms 100.0%
  triton_bmm_131 0.0139 ms 96.6%
  triton_bmm_137 0.0153 ms 87.7%
  triton_bmm_120 0.0169 ms 79.4%
  triton_bmm_141 0.0170 ms 79.1%
  triton_bmm_121 0.0174 ms 77.2%
  bmm 0.0183 ms 73.7%
  triton_bmm_138 0.0189 ms 71.0%
  triton_bmm_139 0.0192 ms 70.0%
  triton_bmm_136 0.0195 ms 69.0%
SingleProcess AUTOTUNE benchmarking takes 2.9836 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0136 ms 100.0%
  triton_bmm_131 0.0147 ms 92.1%
  triton_bmm_137 0.0154 ms 88.2%
  triton_bmm_141 0.0171 ms 79.2%
  triton_bmm_120 0.0172 ms 79.0%
  triton_bmm_121 0.0175 ms 77.6%
  bmm 0.0177 ms 76.5%
  triton_bmm_138 0.0191 ms 70.9%
  triton_bmm_139 0.0194 ms 69.9%
  triton_bmm_136 0.0195 ms 69.6%
SingleProcess AUTOTUNE benchmarking takes 3.0867 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0135 ms 100.0%
  triton_bmm_131 0.0139 ms 97.1%
  triton_bmm_137 0.0153 ms 88.3%
  triton_bmm_141 0.0171 ms 79.3%
  triton_bmm_121 0.0174 ms 77.9%
  bmm 0.0177 ms 76.6%
  triton_bmm_120 0.0179 ms 75.4%
  triton_bmm_138 0.0191 ms 71.0%
  triton_bmm_139 0.0193 ms 70.1%
  triton_bmm_136 0.0194 ms 69.7%
SingleProcess AUTOTUNE benchmarking takes 2.8893 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x2752, 2752x4096)
  mm 0.0631 ms 100.0%
  triton_mm_113 0.0691 ms 91.3%
  triton_mm_96 0.0853 ms 73.9%
  triton_mm_112 0.0879 ms 71.8%
  triton_mm_97 0.0893 ms 70.6%
  triton_mm_107 0.0960 ms 65.7%
  triton_mm_105 0.0972 ms 64.9%
  triton_mm_104 0.1005 ms 62.8%
  triton_mm_118 0.1005 ms 62.8%
  triton_mm_109 0.1016 ms 62.1%
SingleProcess AUTOTUNE benchmarking takes 22.8558 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0131 ms 100.0%
  triton_bmm_131 0.0138 ms 95.3%
  triton_bmm_137 0.0150 ms 87.7%
  triton_bmm_141 0.0167 ms 78.7%
  triton_bmm_120 0.0167 ms 78.5%
  bmm 0.0169 ms 77.5%
  triton_bmm_121 0.0173 ms 76.1%
  triton_bmm_138 0.0187 ms 70.2%
  triton_bmm_139 0.0189 ms 69.3%
  triton_bmm_136 0.0191 ms 68.9%
SingleProcess AUTOTUNE benchmarking takes 2.9825 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0131 ms 100.0%
  triton_bmm_131 0.0136 ms 95.9%
  triton_bmm_137 0.0150 ms 86.9%
  triton_bmm_141 0.0167 ms 78.4%
  triton_bmm_120 0.0167 ms 78.2%
  triton_bmm_121 0.0170 ms 76.9%
  bmm 0.0175 ms 74.4%
  triton_bmm_138 0.0187 ms 69.7%
  triton_bmm_139 0.0190 ms 68.8%
  triton_bmm_136 0.0193 ms 67.6%
SingleProcess AUTOTUNE benchmarking takes 2.8019 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0135 ms 100.0%
  triton_bmm_155 0.0140 ms 96.6%
  triton_bmm_161 0.0153 ms 88.7%
  triton_bmm_144 0.0171 ms 79.3%
  triton_bmm_165 0.0171 ms 79.0%
  bmm 0.0174 ms 77.9%
  triton_bmm_145 0.0174 ms 77.7%
  triton_bmm_162 0.0191 ms 71.0%
  triton_bmm_163 0.0193 ms 70.1%
  triton_bmm_160 0.0193 ms 70.0%
SingleProcess AUTOTUNE benchmarking takes 2.8810 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_133 0.0134 ms 100.0%
  triton_bmm_131 0.0141 ms 95.2%
  triton_bmm_137 0.0153 ms 87.9%
  triton_bmm_141 0.0169 ms 79.4%
  triton_bmm_120 0.0169 ms 79.2%
  triton_bmm_121 0.0172 ms 77.9%
  bmm 0.0176 ms 76.3%
  triton_bmm_138 0.0189 ms 70.8%
  triton_bmm_139 0.0192 ms 69.9%
  triton_bmm_136 0.0195 ms 68.9%
SingleProcess AUTOTUNE benchmarking takes 3.0508 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0136 ms 100.0%
  triton_bmm_155 0.0140 ms 97.1%
  triton_bmm_161 0.0154 ms 88.5%
  triton_bmm_165 0.0171 ms 79.6%
  triton_bmm_144 0.0171 ms 79.4%
  triton_bmm_145 0.0175 ms 78.0%
  bmm 0.0175 ms 78.0%
  triton_bmm_162 0.0191 ms 71.3%
  triton_bmm_163 0.0194 ms 70.2%
  triton_bmm_160 0.0197 ms 69.1%
SingleProcess AUTOTUNE benchmarking takes 2.8897 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0133 ms 100.0%
  triton_bmm_155 0.0138 ms 96.2%
  triton_bmm_161 0.0151 ms 87.8%
  triton_bmm_165 0.0169 ms 78.9%
  triton_bmm_144 0.0170 ms 78.1%
  triton_bmm_145 0.0173 ms 76.9%
  bmm 0.0178 ms 74.6%
  triton_bmm_162 0.0189 ms 70.3%
  triton_bmm_163 0.0191 ms 69.5%
  triton_bmm_160 0.0192 ms 69.2%
SingleProcess AUTOTUNE benchmarking takes 2.8643 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0135 ms 100.0%
  triton_bmm_155 0.0140 ms 96.3%
  triton_bmm_161 0.0153 ms 88.0%
  triton_bmm_165 0.0171 ms 78.7%
  triton_bmm_145 0.0173 ms 78.0%
  triton_bmm_144 0.0178 ms 75.7%
  bmm 0.0180 ms 75.1%
  triton_bmm_162 0.0190 ms 70.9%
  triton_bmm_163 0.0193 ms 69.9%
  triton_bmm_160 0.0196 ms 68.9%
SingleProcess AUTOTUNE benchmarking takes 2.9151 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_179 0.0139 ms 100.0%
  triton_bmm_181 0.0142 ms 98.0%
  triton_bmm_185 0.0153 ms 90.8%
  triton_bmm_168 0.0169 ms 82.0%
  triton_bmm_189 0.0171 ms 81.3%
  triton_bmm_169 0.0173 ms 80.1%
  bmm 0.0183 ms 75.9%
  triton_bmm_186 0.0190 ms 73.1%
  triton_bmm_187 0.0193 ms 72.1%
  triton_bmm_184 0.0194 ms 71.5%
SingleProcess AUTOTUNE benchmarking takes 2.8453 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_181 0.0136 ms 100.0%
  triton_bmm_179 0.0140 ms 97.1%
  triton_bmm_185 0.0154 ms 88.5%
  triton_bmm_189 0.0172 ms 79.1%
  triton_bmm_169 0.0175 ms 77.6%
  bmm 0.0177 ms 76.7%
  triton_bmm_168 0.0181 ms 75.4%
  triton_bmm_186 0.0192 ms 71.0%
  triton_bmm_187 0.0195 ms 70.0%
  triton_bmm_184 0.0195 ms 69.7%
SingleProcess AUTOTUNE benchmarking takes 2.9247 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_181 0.0134 ms 100.0%
  triton_bmm_179 0.0138 ms 96.8%
  triton_bmm_185 0.0152 ms 88.1%
  triton_bmm_189 0.0169 ms 79.0%
  triton_bmm_168 0.0170 ms 78.6%
  triton_bmm_169 0.0173 ms 77.3%
  bmm 0.0177 ms 75.7%
  triton_bmm_186 0.0189 ms 70.6%
  triton_bmm_187 0.0192 ms 69.7%
  triton_bmm_184 0.0194 ms 69.0%
SingleProcess AUTOTUNE benchmarking takes 2.9481 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0131 ms 100.0%
  triton_bmm_155 0.0136 ms 96.5%
  triton_bmm_161 0.0150 ms 87.7%
  triton_bmm_165 0.0169 ms 77.9%
  triton_bmm_144 0.0169 ms 77.7%
  triton_bmm_145 0.0173 ms 75.9%
  bmm 0.0175 ms 75.2%
  triton_bmm_162 0.0187 ms 70.1%
  triton_bmm_163 0.0190 ms 69.1%
  triton_bmm_160 0.0190 ms 69.1%
SingleProcess AUTOTUNE benchmarking takes 2.9468 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0132 ms 100.0%
  triton_bmm_155 0.0136 ms 97.0%
  triton_bmm_161 0.0150 ms 87.7%
  triton_bmm_144 0.0167 ms 78.9%
  triton_bmm_165 0.0167 ms 78.7%
  triton_bmm_145 0.0170 ms 77.4%
  bmm 0.0175 ms 75.3%
  triton_bmm_162 0.0187 ms 70.3%
  triton_bmm_163 0.0190 ms 69.4%
  triton_bmm_160 0.0193 ms 68.3%
SingleProcess AUTOTUNE benchmarking takes 2.9073 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_157 0.0135 ms 100.0%
  triton_bmm_155 0.0139 ms 97.1%
  triton_bmm_161 0.0152 ms 88.4%
  triton_bmm_165 0.0169 ms 79.6%
  triton_bmm_145 0.0173 ms 77.6%
  bmm 0.0175 ms 76.7%
  triton_bmm_144 0.0179 ms 75.2%
  triton_bmm_162 0.0190 ms 70.9%
  triton_bmm_163 0.0192 ms 70.0%
  triton_bmm_160 0.0193 ms 69.7%
SingleProcess AUTOTUNE benchmarking takes 2.9183 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_205 0.0138 ms 100.0%
  triton_bmm_203 0.0140 ms 98.3%
  triton_bmm_209 0.0154 ms 89.3%
  triton_bmm_192 0.0171 ms 80.6%
  triton_bmm_213 0.0171 ms 80.4%
  bmm 0.0175 ms 78.9%
  triton_bmm_193 0.0182 ms 75.6%
  triton_bmm_210 0.0191 ms 72.0%
  triton_bmm_208 0.0197 ms 69.9%
  triton_bmm_199 0.0204 ms 67.6%
SingleProcess AUTOTUNE benchmarking takes 2.9017 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_205 0.0134 ms 100.0%
  triton_bmm_203 0.0138 ms 97.1%
  triton_bmm_209 0.0152 ms 88.1%
  triton_bmm_213 0.0168 ms 79.5%
  triton_bmm_192 0.0169 ms 79.1%
  triton_bmm_193 0.0172 ms 77.7%
  bmm 0.0177 ms 75.6%
  triton_bmm_210 0.0189 ms 70.9%
  triton_bmm_211 0.0191 ms 70.0%
  triton_bmm_208 0.0193 ms 69.2%
SingleProcess AUTOTUNE benchmarking takes 2.8577 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_205 0.0135 ms 100.0%
  triton_bmm_203 0.0139 ms 97.1%
  triton_bmm_209 0.0153 ms 88.0%
  triton_bmm_213 0.0170 ms 79.3%
  triton_bmm_192 0.0170 ms 79.3%
  bmm 0.0173 ms 77.8%
  triton_bmm_193 0.0173 ms 77.8%
  triton_bmm_210 0.0190 ms 71.1%
  triton_bmm_211 0.0192 ms 70.2%
  triton_bmm_208 0.0194 ms 69.6%
SingleProcess AUTOTUNE benchmarking takes 2.9040 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_181 0.0131 ms 100.0%
  triton_bmm_179 0.0137 ms 95.9%
  triton_bmm_185 0.0150 ms 87.2%
  triton_bmm_168 0.0167 ms 78.6%
  triton_bmm_189 0.0167 ms 78.6%
  triton_bmm_169 0.0170 ms 77.1%
  bmm 0.0175 ms 74.7%
  triton_bmm_186 0.0187 ms 70.0%
  triton_bmm_187 0.0189 ms 69.1%
  triton_bmm_184 0.0192 ms 68.1%
SingleProcess AUTOTUNE benchmarking takes 2.9065 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_181 0.0130 ms 100.0%
  triton_bmm_179 0.0136 ms 95.9%
  triton_bmm_185 0.0149 ms 87.1%
  triton_bmm_189 0.0166 ms 78.3%
  triton_bmm_169 0.0170 ms 76.5%
  bmm 0.0175 ms 74.4%
  triton_bmm_168 0.0175 ms 74.4%
  triton_bmm_186 0.0187 ms 69.6%
  triton_bmm_184 0.0194 ms 67.1%
  triton_bmm_175 0.0199 ms 65.4%
SingleProcess AUTOTUNE benchmarking takes 2.9382 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_229 0.0136 ms 100.0%
  triton_bmm_227 0.0140 ms 96.9%
  triton_bmm_233 0.0154 ms 88.3%
  triton_bmm_237 0.0172 ms 79.0%
  triton_bmm_217 0.0175 ms 77.8%
  bmm 0.0177 ms 76.5%
  triton_bmm_216 0.0181 ms 75.2%
  triton_bmm_234 0.0191 ms 70.9%
  triton_bmm_235 0.0194 ms 70.0%
  triton_bmm_232 0.0195 ms 69.5%
SingleProcess AUTOTUNE benchmarking takes 2.8969 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_229 0.0133 ms 100.0%
  triton_bmm_227 0.0138 ms 96.5%
  triton_bmm_233 0.0152 ms 87.4%
  triton_bmm_237 0.0169 ms 78.9%
  triton_bmm_217 0.0173 ms 77.0%
  bmm 0.0178 ms 74.6%
  triton_bmm_216 0.0178 ms 74.6%
  triton_bmm_234 0.0189 ms 70.3%
  triton_bmm_235 0.0191 ms 69.5%
  triton_bmm_232 0.0195 ms 68.3%
SingleProcess AUTOTUNE benchmarking takes 2.8713 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_205 0.0133 ms 100.0%
  triton_bmm_203 0.0136 ms 97.6%
  triton_bmm_209 0.0150 ms 88.5%
  triton_bmm_192 0.0167 ms 79.4%
  triton_bmm_213 0.0168 ms 79.0%
  triton_bmm_193 0.0171 ms 77.7%
  bmm 0.0175 ms 75.6%
  triton_bmm_210 0.0187 ms 70.7%
  triton_bmm_211 0.0189 ms 70.0%
  triton_bmm_208 0.0191 ms 69.4%
SingleProcess AUTOTUNE benchmarking takes 2.8510 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_253 0.0138 ms 100.0%
  triton_bmm_251 0.0141 ms 98.0%
  triton_bmm_257 0.0155 ms 89.1%
  triton_bmm_240 0.0172 ms 80.0%
  triton_bmm_261 0.0173 ms 79.8%
  bmm 0.0175 ms 78.9%
  triton_bmm_241 0.0175 ms 78.7%
  triton_bmm_258 0.0192 ms 71.7%
  triton_bmm_259 0.0195 ms 70.8%
  triton_bmm_256 0.0197 ms 69.8%
SingleProcess AUTOTUNE benchmarking takes 3.1981 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_253 0.0135 ms 100.0%
  triton_bmm_251 0.0139 ms 96.8%
  triton_bmm_257 0.0152 ms 88.7%
  triton_bmm_240 0.0170 ms 79.5%
  triton_bmm_261 0.0171 ms 79.1%
  triton_bmm_241 0.0173 ms 77.8%
  bmm 0.0177 ms 76.3%
  triton_bmm_258 0.0190 ms 70.9%
  triton_bmm_259 0.0192 ms 70.2%
  triton_bmm_256 0.0193 ms 69.9%
SingleProcess AUTOTUNE benchmarking takes 2.9101 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_253 0.0130 ms 100.0%
  triton_bmm_251 0.0136 ms 95.9%
  triton_bmm_257 0.0150 ms 86.9%
  triton_bmm_261 0.0166 ms 78.3%
  triton_bmm_240 0.0167 ms 77.8%
  triton_bmm_241 0.0171 ms 76.1%
  bmm 0.0175 ms 74.4%
  triton_bmm_258 0.0187 ms 69.6%
  triton_bmm_259 0.0189 ms 68.7%
  triton_bmm_256 0.0191 ms 68.3%
SingleProcess AUTOTUNE benchmarking takes 2.9732 seconds and 0.0000 seconds precompiling
AUTOTUNE bmm(8x128x128, 8x128x128)
  triton_bmm_277 0.0137 ms 100.0%
  triton_bmm_275 0.0139 ms 98.9%
  triton_bmm_281 0.0157 ms 87.5%
  triton_bmm_264 0.0170 ms 80.7%
  triton_bmm_285 0.0171 ms 80.5%
  triton_bmm_265 0.0173 ms 79.4%
  bmm 0.0175 ms 78.5%
  triton_bmm_282 0.0190 ms 72.4%
  triton_bmm_283 0.0193 ms 71.2%
  triton_bmm_280 0.0195 ms 70.6%
SingleProcess AUTOTUNE benchmarking takes 2.8754 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3963 ms 100.0%
  triton_mm_887 0.4562 ms 86.9%
  triton_mm_886 0.4958 ms 79.9%
  triton_mm_881 0.5785 ms 68.5%
  triton_mm_868 0.5794 ms 68.4%
  triton_mm_869 0.5974 ms 66.3%
  triton_mm_872 0.6118 ms 64.8%
  triton_mm_864 0.6235 ms 63.6%
  triton_mm_873 0.6346 ms 62.5%
  triton_mm_865 0.6538 ms 60.6%
SingleProcess AUTOTUNE benchmarking takes 22.1866 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3968 ms 100.0%
  triton_mm_887 0.4575 ms 86.7%
  triton_mm_886 0.4949 ms 80.2%
  triton_mm_881 0.5729 ms 69.3%
  triton_mm_868 0.5783 ms 68.6%
  triton_mm_869 0.5965 ms 66.5%
  triton_mm_872 0.6101 ms 65.0%
  triton_mm_864 0.6221 ms 63.8%
  triton_mm_873 0.6340 ms 62.6%
  triton_mm_865 0.6517 ms 60.9%
SingleProcess AUTOTUNE benchmarking takes 22.2633 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3952 ms 100.0%
  triton_mm_887 0.4603 ms 85.9%
  triton_mm_886 0.4955 ms 79.8%
  triton_mm_868 0.5779 ms 68.4%
  triton_mm_881 0.5798 ms 68.1%
  triton_mm_869 0.5963 ms 66.3%
  triton_mm_872 0.6109 ms 64.7%
  triton_mm_864 0.6231 ms 63.4%
  triton_mm_873 0.6354 ms 62.2%
  triton_mm_865 0.6519 ms 60.6%
SingleProcess AUTOTUNE benchmarking takes 22.2519 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3930 ms 100.0%
  triton_mm_887 0.4602 ms 85.4%
  triton_mm_886 0.4943 ms 79.5%
  triton_mm_881 0.5111 ms 76.9%
  triton_mm_868 0.5769 ms 68.1%
  triton_mm_869 0.5938 ms 66.2%
  triton_mm_872 0.6100 ms 64.4%
  triton_mm_864 0.6204 ms 63.4%
  triton_mm_873 0.6325 ms 62.1%
  triton_mm_865 0.6549 ms 60.0%
SingleProcess AUTOTUNE benchmarking takes 25.9932 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3987 ms 100.0%
  triton_mm_887 0.4598 ms 86.7%
  triton_mm_886 0.4938 ms 80.7%
  triton_mm_881 0.5757 ms 69.2%
  triton_mm_868 0.5777 ms 69.0%
  triton_mm_869 0.5944 ms 67.1%
  triton_mm_872 0.6096 ms 65.4%
  triton_mm_864 0.6218 ms 64.1%
  triton_mm_873 0.6326 ms 63.0%
  triton_mm_865 0.6560 ms 60.8%
SingleProcess AUTOTUNE benchmarking takes 22.2214 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3956 ms 100.0%
  triton_mm_887 0.4537 ms 87.2%
  triton_mm_886 0.4938 ms 80.1%
  triton_mm_881 0.5714 ms 69.2%
  triton_mm_868 0.5782 ms 68.4%
  triton_mm_869 0.5972 ms 66.2%
  triton_mm_872 0.6098 ms 64.9%
  triton_mm_864 0.6230 ms 63.5%
  triton_mm_873 0.6323 ms 62.6%
  triton_mm_865 0.6525 ms 60.6%
SingleProcess AUTOTUNE benchmarking takes 22.6607 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3921 ms 100.0%
  triton_mm_887 0.4584 ms 85.5%
  triton_mm_886 0.4928 ms 79.6%
  triton_mm_868 0.5777 ms 67.9%
  triton_mm_881 0.5799 ms 67.6%
  triton_mm_869 0.6010 ms 65.2%
  triton_mm_872 0.6104 ms 64.2%
  triton_mm_864 0.6223 ms 63.0%
  triton_mm_873 0.6339 ms 61.9%
  triton_mm_865 0.6536 ms 60.0%
SingleProcess AUTOTUNE benchmarking takes 24.9810 seconds and 0.0000 seconds precompiling
AUTOTUNE mm(128x4096, 4096x37984)
  mm 0.3968 ms 100.0%
  triton_mm_887 0.4562 ms 87.0%
  triton_mm_886 0.4943 ms 80.3%
  triton_mm_881 0.5699 ms 69.6%
  triton_mm_868 0.5795 ms 68.5%
  triton_mm_869 0.5978 ms 66.4%
  triton_mm_872 0.6097 ms 65.1%
  triton_mm_864 0.6225 ms 63.7%
  triton_mm_873 0.6342 ms 62.6%
  triton_mm_865 0.6513 ms 60.9%
SingleProcess AUTOTUNE benchmarking takes 25.4807 seconds and 0.0000 seconds precompiling
 iteration        1/    2000 | consumed samples:          128 | elapsed time per iteration (ms): 334651.5 | learning rate: 1.000E-06 | global batch size:   128 | lm loss: 3.568386E+00 | loss scale: 1.0 | grad norm: 162.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 22162.9638671875 | max allocated: 22162.9951171875 | reserved: 22634.0 | max reserved: 22634.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 22162.9638671875 | max allocated: 22162.9951171875 | reserved: 22634.0 | max reserved: 22634.0[Rank 1] (after 1 iterations) memory (MB) | allocated: 22162.9638671875 | max allocated: 22162.9951171875 | reserved: 22634.0 | max reserved: 22634.0

[Rank 2] (after 1 iterations) memory (MB) | allocated: 22162.9638671875 | max allocated: 22162.9951171875 | reserved: 22634.0 | max reserved: 22634.0
 iteration        2/    2000 | consumed samples:          256 | elapsed time per iteration (ms): 11037.3 | learning rate: 2.000E-06 | global batch size:   128 | lm loss: 3.665083E+00 | loss scale: 1.0 | grad norm: 95.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/    2000 | consumed samples:          384 | elapsed time per iteration (ms): 11920.2 | learning rate: 3.000E-06 | global batch size:   128 | lm loss: 3.319508E+00 | loss scale: 1.0 | grad norm: 86.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/    2000 | consumed samples:          512 | elapsed time per iteration (ms): 9707.8 | learning rate: 4.000E-06 | global batch size:   128 | lm loss: 3.540865E+00 | loss scale: 1.0 | grad norm: 84.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/    2000 | consumed samples:          640 | elapsed time per iteration (ms): 9755.4 | learning rate: 5.000E-06 | global batch size:   128 | lm loss: 3.745203E+00 | loss scale: 1.0 | grad norm: 106.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/    2000 | consumed samples:          768 | elapsed time per iteration (ms): 9620.3 | learning rate: 6.000E-06 | global batch size:   128 | lm loss: 3.019699E+00 | loss scale: 1.0 | grad norm: 73.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/    2000 | consumed samples:          896 | elapsed time per iteration (ms): 10736.2 | learning rate: 7.000E-06 | global batch size:   128 | lm loss: 3.266235E+00 | loss scale: 1.0 | grad norm: 50.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/    2000 | consumed samples:         1024 | elapsed time per iteration (ms): 9774.8 | learning rate: 8.000E-06 | global batch size:   128 | lm loss: 2.748482E+00 | loss scale: 1.0 | grad norm: 39.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/    2000 | consumed samples:         1152 | elapsed time per iteration (ms): 10542.9 | learning rate: 9.000E-06 | global batch size:   128 | lm loss: 2.629471E+00 | loss scale: 1.0 | grad norm: 36.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/    2000 | consumed samples:         1280 | elapsed time per iteration (ms): 10056.1 | learning rate: 1.000E-05 | global batch size:   128 | lm loss: 2.776215E+00 | loss scale: 1.0 | grad norm: 42.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/    2000 | consumed samples:         1408 | elapsed time per iteration (ms): 9549.1 | learning rate: 9.995E-06 | global batch size:   128 | lm loss: 2.311481E+00 | loss scale: 1.0 | grad norm: 30.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/    2000 | consumed samples:         1536 | elapsed time per iteration (ms): 9529.1 | learning rate: 9.991E-06 | global batch size:   128 | lm loss: 2.596030E+00 | loss scale: 1.0 | grad norm: 33.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/    2000 | consumed samples:         1664 | elapsed time per iteration (ms): 9953.8 | learning rate: 9.986E-06 | global batch size:   128 | lm loss: 2.279638E+00 | loss scale: 1.0 | grad norm: 41.236 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/    2000 | consumed samples:         1792 | elapsed time per iteration (ms): 10175.3 | learning rate: 9.982E-06 | global batch size:   128 | lm loss: 2.350719E+00 | loss scale: 1.0 | grad norm: 23.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/    2000 | consumed samples:         1920 | elapsed time per iteration (ms): 9928.6 | learning rate: 9.977E-06 | global batch size:   128 | lm loss: 2.309711E+00 | loss scale: 1.0 | grad norm: 28.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/    2000 | consumed samples:         2048 | elapsed time per iteration (ms): 9908.8 | learning rate: 9.973E-06 | global batch size:   128 | lm loss: 2.299427E+00 | loss scale: 1.0 | grad norm: 22.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/    2000 | consumed samples:         2176 | elapsed time per iteration (ms): 9984.4 | learning rate: 9.968E-06 | global batch size:   128 | lm loss: 2.382552E+00 | loss scale: 1.0 | grad norm: 28.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/    2000 | consumed samples:         2304 | elapsed time per iteration (ms): 9373.7 | learning rate: 9.964E-06 | global batch size:   128 | lm loss: 2.057923E+00 | loss scale: 1.0 | grad norm: 17.633 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/    2000 | consumed samples:         2432 | elapsed time per iteration (ms): 9935.4 | learning rate: 9.959E-06 | global batch size:   128 | lm loss: 2.265843E+00 | loss scale: 1.0 | grad norm: 25.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/    2000 | consumed samples:         2560 | elapsed time per iteration (ms): 10057.4 | learning rate: 9.955E-06 | global batch size:   128 | lm loss: 2.158366E+00 | loss scale: 1.0 | grad norm: 24.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/    2000 | consumed samples:         2688 | elapsed time per iteration (ms): 9882.4 | learning rate: 9.950E-06 | global batch size:   128 | lm loss: 2.086797E+00 | loss scale: 1.0 | grad norm: 16.789 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/    2000 | consumed samples:         2816 | elapsed time per iteration (ms): 9292.1 | learning rate: 9.945E-06 | global batch size:   128 | lm loss: 1.991774E+00 | loss scale: 1.0 | grad norm: 16.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/    2000 | consumed samples:         2944 | elapsed time per iteration (ms): 9603.2 | learning rate: 9.941E-06 | global batch size:   128 | lm loss: 1.980265E+00 | loss scale: 1.0 | grad norm: 13.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/    2000 | consumed samples:         3072 | elapsed time per iteration (ms): 9424.5 | learning rate: 9.936E-06 | global batch size:   128 | lm loss: 2.153835E+00 | loss scale: 1.0 | grad norm: 19.079 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/    2000 | consumed samples:         3200 | elapsed time per iteration (ms): 10867.6 | learning rate: 9.932E-06 | global batch size:   128 | lm loss: 2.038693E+00 | loss scale: 1.0 | grad norm: 17.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/    2000 | consumed samples:         3328 | elapsed time per iteration (ms): 10322.6 | learning rate: 9.927E-06 | global batch size:   128 | lm loss: 2.008749E+00 | loss scale: 1.0 | grad norm: 17.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/    2000 | consumed samples:         3456 | elapsed time per iteration (ms): 9449.2 | learning rate: 9.923E-06 | global batch size:   128 | lm loss: 2.136619E+00 | loss scale: 1.0 | grad norm: 18.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/    2000 | consumed samples:         3584 | elapsed time per iteration (ms): 9405.7 | learning rate: 9.918E-06 | global batch size:   128 | lm loss: 1.974890E+00 | loss scale: 1.0 | grad norm: 15.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/    2000 | consumed samples:         3712 | elapsed time per iteration (ms): 9357.1 | learning rate: 9.914E-06 | global batch size:   128 | lm loss: 2.019541E+00 | loss scale: 1.0 | grad norm: 16.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/    2000 | consumed samples:         3840 | elapsed time per iteration (ms): 9854.7 | learning rate: 9.909E-06 | global batch size:   128 | lm loss: 1.873558E+00 | loss scale: 1.0 | grad norm: 16.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/    2000 | consumed samples:         3968 | elapsed time per iteration (ms): 9950.4 | learning rate: 9.905E-06 | global batch size:   128 | lm loss: 1.917818E+00 | loss scale: 1.0 | grad norm: 19.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/    2000 | consumed samples:         4096 | elapsed time per iteration (ms): 10398.0 | learning rate: 9.900E-06 | global batch size:   128 | lm loss: 2.056309E+00 | loss scale: 1.0 | grad norm: 15.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/    2000 | consumed samples:         4224 | elapsed time per iteration (ms): 9943.0 | learning rate: 9.895E-06 | global batch size:   128 | lm loss: 1.962593E+00 | loss scale: 1.0 | grad norm: 18.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/    2000 | consumed samples:         4352 | elapsed time per iteration (ms): 9375.9 | learning rate: 9.891E-06 | global batch size:   128 | lm loss: 2.056561E+00 | loss scale: 1.0 | grad norm: 15.218 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/    2000 | consumed samples:         4480 | elapsed time per iteration (ms): 9501.4 | learning rate: 9.886E-06 | global batch size:   128 | lm loss: 2.216560E+00 | loss scale: 1.0 | grad norm: 17.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/    2000 | consumed samples:         4608 | elapsed time per iteration (ms): 9984.1 | learning rate: 9.882E-06 | global batch size:   128 | lm loss: 1.885576E+00 | loss scale: 1.0 | grad norm: 14.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/    2000 | consumed samples:         4736 | elapsed time per iteration (ms): 9369.2 | learning rate: 9.877E-06 | global batch size:   128 | lm loss: 1.930587E+00 | loss scale: 1.0 | grad norm: 15.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/    2000 | consumed samples:         4864 | elapsed time per iteration (ms): 10509.8 | learning rate: 9.873E-06 | global batch size:   128 | lm loss: 1.996860E+00 | loss scale: 1.0 | grad norm: 13.075 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/    2000 | consumed samples:         4992 | elapsed time per iteration (ms): 9875.9 | learning rate: 9.868E-06 | global batch size:   128 | lm loss: 2.092668E+00 | loss scale: 1.0 | grad norm: 17.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/    2000 | consumed samples:         5120 | elapsed time per iteration (ms): 9891.9 | learning rate: 9.864E-06 | global batch size:   128 | lm loss: 2.005273E+00 | loss scale: 1.0 | grad norm: 15.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/    2000 | consumed samples:         5248 | elapsed time per iteration (ms): 9895.1 | learning rate: 9.859E-06 | global batch size:   128 | lm loss: 1.898225E+00 | loss scale: 1.0 | grad norm: 14.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/    2000 | consumed samples:         5376 | elapsed time per iteration (ms): 9497.6 | learning rate: 9.855E-06 | global batch size:   128 | lm loss: 1.964563E+00 | loss scale: 1.0 | grad norm: 16.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/    2000 | consumed samples:         5504 | elapsed time per iteration (ms): 10032.6 | learning rate: 9.850E-06 | global batch size:   128 | lm loss: 1.947147E+00 | loss scale: 1.0 | grad norm: 19.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/    2000 | consumed samples:         5632 | elapsed time per iteration (ms): 9852.5 | learning rate: 9.845E-06 | global batch size:   128 | lm loss: 2.042275E+00 | loss scale: 1.0 | grad norm: 15.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/    2000 | consumed samples:         5760 | elapsed time per iteration (ms): 9551.3 | learning rate: 9.841E-06 | global batch size:   128 | lm loss: 2.025980E+00 | loss scale: 1.0 | grad norm: 33.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/    2000 | consumed samples:         5888 | elapsed time per iteration (ms): 9478.7 | learning rate: 9.836E-06 | global batch size:   128 | lm loss: 1.971150E+00 | loss scale: 1.0 | grad norm: 16.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/    2000 | consumed samples:         6016 | elapsed time per iteration (ms): 9428.6 | learning rate: 9.832E-06 | global batch size:   128 | lm loss: 2.025551E+00 | loss scale: 1.0 | grad norm: 14.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/    2000 | consumed samples:         6144 | elapsed time per iteration (ms): 10844.4 | learning rate: 9.827E-06 | global batch size:   128 | lm loss: 1.917778E+00 | loss scale: 1.0 | grad norm: 13.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/    2000 | consumed samples:         6272 | elapsed time per iteration (ms): 10392.7 | learning rate: 9.823E-06 | global batch size:   128 | lm loss: 1.937559E+00 | loss scale: 1.0 | grad norm: 15.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/    2000 | consumed samples:         6400 | elapsed time per iteration (ms): 9685.7 | learning rate: 9.818E-06 | global batch size:   128 | lm loss: 1.887969E+00 | loss scale: 1.0 | grad norm: 12.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/    2000 | consumed samples:         6528 | elapsed time per iteration (ms): 9294.9 | learning rate: 9.814E-06 | global batch size:   128 | lm loss: 2.047933E+00 | loss scale: 1.0 | grad norm: 15.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/    2000 | consumed samples:         6656 | elapsed time per iteration (ms): 9554.4 | learning rate: 9.809E-06 | global batch size:   128 | lm loss: 1.954652E+00 | loss scale: 1.0 | grad norm: 17.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/    2000 | consumed samples:         6784 | elapsed time per iteration (ms): 9489.3 | learning rate: 9.805E-06 | global batch size:   128 | lm loss: 1.885500E+00 | loss scale: 1.0 | grad norm: 14.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/    2000 | consumed samples:         6912 | elapsed time per iteration (ms): 10605.8 | learning rate: 9.800E-06 | global batch size:   128 | lm loss: 1.895607E+00 | loss scale: 1.0 | grad norm: 22.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/    2000 | consumed samples:         7040 | elapsed time per iteration (ms): 10300.5 | learning rate: 9.795E-06 | global batch size:   128 | lm loss: 1.796477E+00 | loss scale: 1.0 | grad norm: 13.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/    2000 | consumed samples:         7168 | elapsed time per iteration (ms): 9990.7 | learning rate: 9.791E-06 | global batch size:   128 | lm loss: 1.893637E+00 | loss scale: 1.0 | grad norm: 12.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/    2000 | consumed samples:         7296 | elapsed time per iteration (ms): 9430.5 | learning rate: 9.786E-06 | global batch size:   128 | lm loss: 1.848465E+00 | loss scale: 1.0 | grad norm: 12.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/    2000 | consumed samples:         7424 | elapsed time per iteration (ms): 9576.9 | learning rate: 9.782E-06 | global batch size:   128 | lm loss: 1.861838E+00 | loss scale: 1.0 | grad norm: 16.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/    2000 | consumed samples:         7552 | elapsed time per iteration (ms): 9834.3 | learning rate: 9.777E-06 | global batch size:   128 | lm loss: 2.031286E+00 | loss scale: 1.0 | grad norm: 17.234 | number of skipped iterations:   0 | number of nan iterations:   0 |
